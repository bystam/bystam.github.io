---
layout: post
title: "Are we in a software bubble?"
date: 2030-11-30 11:00:44 +0200
categories: takes
---

There is something that has been bugging me in the back of my mind for the past few months. Some kind of feeling that what I am hearing and reading is not what is truly happening. Yes, this is yet another blog post about AI. Hopefully one that contains something novel.

As I said - I feel like there is something brewing - something people are not talking about. This is not about people being fraudulent or gaslighting the public, although I suspect there is some amount of that going on too. A lot of people talk about the "AI bubble" right now. Anecdotally, it feels like a majority of people in tech agree/suspect that there is one. Even a substantial amount of people who love LLMs and have close to completely stopped writing code by hand will admit that there likely is some level of bubble in the industry.

What I am wondering, however, is if that is missing the mark. What if we're not in an AI bubble - **but a software one**?

## The great promises of AI

LLMs and the tools built on top of them are an obvious breakthrough in digital technology. Their ability to "understand" text is unparalleled, and using dialogue to research and understand topics are striking improvements when compared to reading the internet through Google. Especially since the years leading up to the release of ChatGPT saw an absoultely horrendous enshittification of what used to be a remarkable search engine.

The market hype responded accordingly. Nvidia, a company that literally creates a single kind of hardware, rose to become the most valuable company in the world - and AI-native companies like Lovable are valued at [ridiculous sums](https://lovable.dev/blog/series-b) after only a single year of existence.

The writing on the wall is clear: at some point in the near future, LLMs/AI will create more value for society than we can possibly imagine. Or, that's what the investments tell us anyway.

### The AI sceptics

The sceptics will tell you that all of this is a bubble:

- LLMs are not as good as they appear. They just feel like it because they are generating text, which creates an "illusion of intelligence".
- LLMs, while they can surely generate code, cannot generate good enough code at a large scale to be truly valuable
- LLMs is a crutch that lets people avoid learning

You have heard these takes before. The people bearish on AI think that the LLMs will fail to live up to the hype, and that the markets will crash. This is nothing new.

But that is not what is bugging me. What I am starting to wonder is this: What if there is nothing wrong with the LLMs capabilities. **What if we are simply unable to build anything useful with them?**

Let me explain what I mean.

### The trajectory

Think of the significant software you use today. Then think about all the software you used 10 years ago. How much has changed? For me, not a lot. For work, I use Slack, Google Drive/Docs, Jetbrains IDEs for programming and Google Chrome for web browsing. I used Spotify for listening to music, Netflix for streaming movies and TV shows. On my smartphone, I use Facebook messenger, WhatsApp, Twitter and Youtube.

In 2016, literally all of that was the same. The only new significant piece of software during that decade are the LLM tools.

Now compare that to the 10 years prior, between 2006 and 2016. In 2006, most of the apps listed either did not exist or were in their infancy. Smartphones had not yet hit the mainstream, there were no app stores. Google has not yet released a web browser.

Tiktok is an obvious omission from my list (since I don't use it) that actually was released in the past 10 years. But with the evidence that is connecting social media like Tiktok with severe mental health problems in kids, and with the US congress even going so far as considering it a serious threat to national security, can we really call it useful?

If we look at the last 20 years, there seems to be an extremely visible plateau in the production of useful software. It appears that all our ideas for creating a better world using computers have stagnated.

- TODO: insert examples of Windows, Liquid glass etc

Now, you might think that I am being unfair. During the first of those two decades, an entire computer format (mobile smartphones) was brought to market, and massive online ecosystems such as social media platforms were brought to life. It is only natural that there were significantly more software being built between 2006 and 2016 than the decade after that.

Also - LLMs and AI, you could say, **is one of those new platforms**. So just you wait. In 2036, you will have replaced all your software again.

But... does it look like we are headed that way?

### AI for building new user experiences

Let us compare AI to the expansion of the internet or the release of the smartphone. The argument then would be that AI as a technology will enable entirely new ways to interact with computers, which will open the door to sizable new ways to create value and improve the lives of people.

ChatGPT has been out for over three years now - and several of its open and closed competitors were not far behind. Have we seen that sweeping revolution in computer interactions spring to life?

In comparison, the iPhone was released in 2008 - and the App Store (including the tools to build your own apps) one year after that. In 2009, you had the very first opportunity to install custom apps on your phone - with apps like Spotify being available within weeks. By 2012, it felt like everyday computer interactions had radically shifted to be a mobile thing. Social media had become something you do on your smartphone, you could send money to your friends and family on the go, and various chat apps had completely replaced SMS.

Except for the actual big chatbot companies (OpenAI, Google etc) - where is all the amazing LLM powered functionality? Virtually all companies I see have tried - and for the most part people despise their attempts. Microsoft is being dragged in various online spaces for their attempts to squeeze Copilot into everything. Even famous AI-influencers (in the programming space) bemoan the use of AI in various products:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Every time I see an AI button in a UI somewhere I cringe and ignore it. Simultaneously I’m going nuts with my agents. Why is consumer AI so shit? <a href="https://t.co/6tVxCFCabR">https://t.co/6tVxCFCabR</a></p>&mdash; Armin Ronacher ⇌ (@mitsuhiko) <a href="https://twitter.com/mitsuhiko/status/2010406878870643086?ref_src=twsrc%5Etfw">January 11, 2026</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

I concede that the AI platforms themselves have successfully built useful products on top of these technologies. But so far, it does not appear to scale to the entire industry. Not in the ways the smartphone or the internet did, unless you are a "prompt your way to a website"-company.

### AI as a productivity tool

Another way people expect AI to revolutionize work is as a productivity tool. Especially as a means of generating code.

Sam Altman began his stardom as the face of AI by declaring his intent for AI to cure cancer or make novel discoveries in the field of physics. Despite that, it looks like most frontier model companies spend a lot of their time shipping products dedicated to "agentic programming". Products like Claude Code have become so popular that now even political journalists talk about it:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">AI continues to cause my productivity to plummet because I keep messing around with Claude Code rather than actually writing articles. <a href="https://t.co/oYmfjBBgJT">pic.twitter.com/oYmfjBBgJT</a></p>&mdash; Matthew Yglesias (@mattyglesias) <a href="https://twitter.com/mattyglesias/status/2009656349744087283?ref_src=twsrc%5Etfw">January 9, 2026</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Last year was a bit of a watershed moment for LLM-assisted programming. With the release of Opus 4.5 and ChatGPT 5.2, a serious amount of independently minded software influencers changed their mind from "AI cannot write good production code" to "actually AI writes most of my code now".

To me it looks like the tide is turning. Maybe this is not just an astroturfed fad that will pass in a year or two. Maybe most people will truly write code using "Agents" (or whatever comes next) in the near future. Heck, what if the models get good enough to simply help all of us write just as good code we would write by hand - but at a significantly higher pace. If that happens, then the AI hypesters will have won, right?

But here is where I go back to where I started this article. Look at the direction we have been headed in the past decade. Will accelerating our productivity in said direction... actually lead to a lot of valuable software?

Even the most crazed, bullish AI influencers online are all about how "AI can write **as good code as senior developers**, but faster". I rarely hear them talk about, or expect, that LLMs would transcend human talent and surpass us when it comes to software quality. Given that LLMs are trained on all of our existing code - making it become significantly better seems like a tricky problem to solve. If it is even solvable with the existing model architectures.

So if the wildest dreams of Dario Amodei come true, and we boost or productivity writing code tenfold, are we certain we won't just spew out even more irrelevant SaaS garbage? Even more blur animations in our Finder windows that cause even harder frame rate drops? Even more addictive, attention-hacking software that [ruin the brains of our kids](https://news.ki.se/using-social-media-may-impair-childrens-attention) and make them [suicidal](https://www.cdc.gov/mmwr/volumes/73/su/su7304a3.htm)?

If AI is the productivity powerhouse that its proponents claim - are we actually in a place where we know what we would do with it? Or has software development seriously stagnated in the past decade? Maybe Casey Muratori is right when he says that it is ironic that we managed to get an AI that replicates our programming at a time when software development standards are at an absolute rock bottom.

## Learnings from the internet
